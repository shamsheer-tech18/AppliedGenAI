{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamsheer-tech18/AppliedGenAI/blob/main/M6_Lab1_AI_Agents_updated.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5208c400",
      "metadata": {
        "id": "5208c400"
      },
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 20px; margin-bottom: 15px; text-align: center; border-radius: 10px;\">\n",
        "<h1 style=\"font-size: 28px; margin-bottom: 8px;\">LangChain: Agents and Chains</h1>\n",
        "<div style=\"background: white; color: #0055d4; padding: 4px 12px; border-radius: 10px; font-size: 13px; display: inline-block; margin-bottom: 8px;\">Prof. Dehghani</div>\n",
        "<p style=\"margin: 0; font-size: 14px;\">m.dehghani@northeastern.edu</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; border-radius: 10px; padding: 15px; margin-bottom: 15px; border: 1px solid #0055d4;\">\n",
        "<h2 style=\"color: #0055d4; margin-top: 0; font-size: 20px; padding-bottom: 8px; border-bottom: 2px solid #0055d4;\">Lab Overview</h2>\n",
        "<p style=\"line-height: 1.6; font-size: 15px; margin-bottom: 10px;\">This lab focuses on automating multi-step reasoning and decision-making in LangChain using <strong>Chains & Agents</strong>. You'll learn how to connect multiple components, dynamically execute logic, and use LLMs to make decisions.</p>\n",
        "\n",
        "<div style=\"margin-top: 15px; padding-left: 15px; border-left: 4px solid #0055d4;\">\n",
        "<h3 style=\"color: #0055d4; font-size: 16px; margin-bottom: 10px;\">Learning Objectives</h3>\n",
        "<div style=\"background: white; border-radius: 8px; padding: 12px; margin-bottom: 8px; border: 1px solid #e0e0e0;\">\n",
        "<span style=\"color: #0055d4; font-weight: bold;\">1.</span> Chains in LangChain ‚Äî Connect multiple LLM calls in a sequence\n",
        "</div>\n",
        "<div style=\"background: white; border-radius: 8px; padding: 12px; margin-bottom: 8px; border: 1px solid #e0e0e0;\">\n",
        "<span style=\"color: #0055d4; font-weight: bold;\">2.</span> Agents & Tools ‚Äî Create AI-driven agents that reason & act dynamically\n",
        "</div>\n",
        "<div style=\"background: white; border-radius: 8px; padding: 12px; margin-bottom: 8px; border: 1px solid #e0e0e0;\">\n",
        "<span style=\"color: #0055d4; font-weight: bold;\">3.</span> Hands-on Implementation ‚Äî Apply concepts through practical coding exercises\n",
        "</div>\n",
        "</div>\n",
        "\n",
        "<p style=\"margin-top: 12px; font-size: 14px; color: #666;\">Upon completion, you'll be equipped to build AI workflows using structured pipelines and autonomous decision-making.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e0f1adbd",
      "metadata": {
        "id": "e0f1adbd"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è Installing Required Libraries (Quiet Mode)\n",
        "# ==================================================\n",
        "!pip install -q --upgrade langchain  # Core framework for LLMs\n",
        "!pip install -q --upgrade langchain-community  # Community LLMs, tools, memory, etc.\n",
        "!pip install -q --upgrade openai  # OpenAI API (always use latest unless you have a reason to pin)\n",
        "!pip install -q --upgrade langchain-openai # Install the OpenAI integration for LangChain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade langchain langchain-openai langchain-community langchain-core"
      ],
      "metadata": {
        "id": "oTM-XjlmQS8D"
      },
      "id": "oTM-XjlmQS8D",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain-classic langchain-openai langchain-core #Since legacy MEMORY and CHAINS package moved to this new bridge package"
      ],
      "metadata": {
        "id": "Gg_YhTcORM0o"
      },
      "id": "Gg_YhTcORM0o",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9f6983d2",
      "metadata": {
        "id": "9f6983d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de42b7d8-8743-4ee0-fbf4-690ea7560feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Essential libraries imported successfully from classic bridge!\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# üêç Importing Essential Python Libraries for LangChain + OpenAI\n",
        "# ==================================================\n",
        "\n",
        "#import os  # System: Manage environment variables (e.g., API keys)\n",
        "\n",
        "#import ipywidgets as widgets  # Jupyter: For interactive input controls in notebooks\n",
        "#from IPython.display import clear_output, display  # Jupyter: Output management and display tools\n",
        "\n",
        "#from langchain.memory import ConversationBufferMemory  # LangChain: Stores conversational history for chatbots\n",
        "#from langchain.prompts import PromptTemplate  # LangChain: Create prompt templates for LLMs\n",
        "\n",
        "import os\n",
        "import ipywidgets as widgets # Jupyter: For interactive input controls in notebooks\n",
        "from IPython.display import clear_output, display # Jupyter: Output management and display tools\n",
        "\n",
        "from langchain_openai import ChatOpenAI # LangChain: OpenAI chat model wrapper\n",
        "from langchain_classic.memory import ConversationBufferMemory #classic memory import (fix for previous)\n",
        "from langchain_core.prompts import PromptTemplate #modern core prompt import\n",
        "\n",
        "print(\"‚úÖ Essential libraries imported successfully!!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2d4e47a7",
      "metadata": {
        "id": "2d4e47a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8187f08c-cee1-4ce6-fd22-dd5175d2f772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API Key loaded from secrets!\n"
          ]
        }
      ],
      "source": [
        "# ==================================================\n",
        "# üîí API Key Setup from Colab Secrets\n",
        "# ==================================================\n",
        "\n",
        "from google.colab import userdata  # Colab utility for accessing stored secrets\n",
        "import os  # For environment variables\n",
        "\n",
        "# Read API keys from Colab secrets (assumes you set them via: userdata.set_secret('OPENAI_API_KEY', '...'))\n",
        "openai_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Set as environment variables for downstream packages (LangChain, etc.)\n",
        "if openai_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "    print(\"‚úÖ OpenAI API Key loaded from secrets!\")\n",
        "else:\n",
        "    print(\"‚ùå OpenAI API Key not found in secrets.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"margin-bottom: 25px; padding-left: 15px; border-left: 4px solid #0055d4;\">\n",
        "<h2 style=\"color: #0055d4; margin-top: 0; font-size: 24px; padding-bottom: 10px; border-bottom: 2px solid #e0e0e0;\">Understanding Chains in LangChain</h2>\n",
        "\n",
        "<p style=\"line-height: 1.6; font-size: 16px; color: #666;\">One of the core features of LangChain is its ability to <strong>create chains</strong>, allowing us to sequence multiple tasks together. Instead of manually handling each step, chains automate workflows by linking components such as prompts, LLMs, memory, and tools.</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f8f9fa; border-radius: 10px; padding: 20px; margin-bottom: 20px;\">\n",
        "<h3 style=\"color: #0055d4; margin-top: 0; font-size: 18px; margin-bottom: 15px;\">Why Use Chains in LangChain?</h3>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "<strong style=\"color: #0055d4;\">Automate multi-step processes</strong> ‚Äî No need to manually pass outputs between steps\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "<strong style=\"color: #0055d4;\">Create structured AI pipelines</strong> ‚Äî Chain together LLMs, retrievers, memory, and tools\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "<strong style=\"color: #0055d4;\">Enable decision-making AI agents</strong> ‚Äî Chains help LLMs interact with external tools to retrieve and process information dynamically\n",
        "</div>\n",
        "</div>\n",
        "\n",
        "<div style=\"margin-bottom: 25px; padding-left: 15px; border-left: 4px solid #0055d4;\">\n",
        "<h2 style=\"color: #0055d4; margin-top: 0; font-size: 22px; margin-bottom: 15px;\">Using the Pipe (|) Operator for Cleaner Chaining</h2>\n",
        "\n",
        "<p style=\"line-height: 1.6; font-size: 16px; color: #666; margin-bottom: 15px;\">LangChain provides a simplified way to create chains using the <strong>pipe (|) operator</strong>, which allows direct data flow between components.</p>\n",
        "\n",
        "<h3 style=\"color: #0055d4; font-size: 18px; margin-bottom: 10px;\">Example: Two Ways to Process Input with an LLM</h3>\n",
        "\n",
        "<div style=\"display: flex; gap: 10px; margin-bottom: 15px;\">\n",
        "<div style=\"flex: 1; background: #f0f5ff; padding: 12px; border-radius: 6px; border: 1px solid #e0e0e0;\">\n",
        "<strong style=\"color: #0055d4;\">Without Pipe (|)</strong><br>\n",
        "<span style=\"font-size: 14px; color: #666;\">Manually format the input, then pass it to the LLM</span>\n",
        "</div>\n",
        "\n",
        "<div style=\"flex: 1; background: #f0f5ff; padding: 12px; border-radius: 6px; border: 1px solid #e0e0e0;\">\n",
        "<strong style=\"color: #0055d4;\">With Pipe (|)</strong><br>\n",
        "<span style=\"font-size: 14px; color: #666;\">Directly chain them together for automatic execution</span>\n",
        "</div>\n",
        "</div>\n",
        "\n",
        "<p style=\"font-size: 15px; color: #666;\">Let's compare both approaches in the next code cells!</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "F7lwDEVbbG1v"
      },
      "id": "F7lwDEVbbG1v"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# üîπ **Comparing Two Methods: Manual Execution vs. Pipe (`|`) Operator**\n",
        "# ==================================================\n",
        "# This example demonstrates two ways to process an input with an LLM:\n",
        "#\n",
        "# 1Ô∏è‚É£ **Without Pipe (`|`)** ‚Üí Manually format the prompt and pass it to the LLM.\n",
        "# 2Ô∏è‚É£ **With Pipe (`|`)** ‚Üí Use LangChain‚Äôs `|` operator to create a streamlined sequence.\n",
        "#\n",
        "# Using the `|` operator allows for **cleaner, automatic execution**, reducing code complexity.\n",
        "\n",
        "# ‚úÖ Define a prompt template with a World Cup theme\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"event\"],\n",
        "    template=\"\"\"\n",
        "    You are a legendary sports analyst with deep knowledge of World Cup history.\n",
        "    Fans eagerly await your expert take on the most iconic moments.\n",
        "\n",
        "    Analyze this legendary World Cup event in max ~20 simple words: {event}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Initialize the LLM (GPT-4)\n",
        "llm_ChatGPT = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
        "\n",
        "# ==================================================\n",
        "# ‚ùå Without Using the Pipe (`|`) - More Manual Steps\n",
        "# ==================================================\n",
        "# 1Ô∏è‚É£ Manually format the prompt\n",
        "formatted_prompt = prompt.format(event=\"Zidane's 2006 World Cup final red card\")\n",
        "\n",
        "# 2Ô∏è‚É£ Pass it to the LLM manually\n",
        "response_without_pipe = llm_ChatGPT.invoke(formatted_prompt)\n",
        "\n",
        "# ‚úÖ Print the response (w/ emoji)\n",
        "print(\"üõ†Ô∏è‚ùå Manual (No Pipe) Response:\", response_without_pipe.content)\n",
        "\n",
        "# ==================================================\n",
        "# ‚úÖ Using the Pipe (`|`) - Cleaner and Automatic Execution\n",
        "# ==================================================\n",
        "# 1Ô∏è‚É£ Directly chain the prompt and LLM together\n",
        "chain = prompt | llm_ChatGPT  # This creates a RunnableSequence\n",
        "\n",
        "# 2Ô∏è‚É£ Run the chain with an input in one step\n",
        "response_with_pipe = chain.invoke({\"event\": \"Zidane's 2006 World Cup final red card\"})\n",
        "\n",
        "# ‚úÖ Print the response (w/ emoji)\n",
        "print(\"üöÄ‚úÖ Pipe Operator Response:\", response_with_pipe.content)\n"
      ],
      "metadata": {
        "id": "qboSX5pObOmo"
      },
      "id": "qboSX5pObOmo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úã Hands-On: Single Prompt Chain"
      ],
      "metadata": {
        "id": "jS5bBlCKjE4H"
      },
      "id": "jS5bBlCKjE4H"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ‚úã **Hands-On 1: Creating a Single Prompt Chain**\n",
        "# ==================================================\n",
        "\n",
        "# üìå **Task Instructions:**\n",
        "# 1Ô∏è‚É£ Fill in the missing placeholders (-----) to complete the code.\n",
        "# 2Ô∏è‚É£ Ensure the Prompt Template correctly replaces {topic}.\n",
        "# 3Ô∏è‚É£ Run the code and verify GPT-4 generates a response.\n",
        "\n",
        "# ‚úÖ Step 1: Import necessary modules\n",
        "from langchain.prompts import -----  # Import the correct class\n",
        "from langchain.chat_models import -----  # Import the correct class\n",
        "\n",
        "# ‚úÖ Step 2: Define a Prompt Template\n",
        "prompt_template = -----(\n",
        "    input_variables=[\"-----\"],  # Placeholder for dynamic input\n",
        "    template=\"Explain {topic} in simple terms using no more than 15 words.\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 3: Initialize GPT-4 model\n",
        "llm_ChatGPT = -----(model_name=\"gpt-4\")  # Load GPT-4 model\n",
        "\n",
        "# ‚úÖ Step 4: Create a runnable chain using `|` (pipe operator)\n",
        "chain = prompt_template ----- llm_ChatGPT  # Use the correct operator to chain them\n",
        "\n",
        "# ‚úÖ Step 5: Run the chain with a sample input\n",
        "response = chain.invoke({\"topic\": \"Collective Intelligence\"})\n",
        "\n",
        "# ‚úÖ Step 6: Display results\n",
        "print(\"üîπ **Generated Prompt:**\", prompt_template.format(topic=\"Collective Intelligence\"))\n",
        "print(\"üîπ **LLM Response:**\", response.-----)  # Extract and display response content\n"
      ],
      "metadata": {
        "id": "3WLVBwwAi6Iu"
      },
      "id": "3WLVBwwAi6Iu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"margin-bottom: 25px; padding-left: 15px; border-left: 4px solid #0055d4;\">\n",
        "<h2 style=\"color: #0055d4; margin-top: 0; font-size: 24px; padding-bottom: 10px; border-bottom: 2px solid #e0e0e0;\">Multi-LLM Pipelines with Chaining</h2>\n",
        "\n",
        "<h3 style=\"color: #0055d4; font-size: 18px; margin: 20px 0 10px 0;\">What is Multi-LLM Chaining?</h3>\n",
        "<p style=\"line-height: 1.6; font-size: 16px; color: #666;\">Multi-LLM chaining is a method where <strong>multiple AI models</strong> collaborate in a step-by-step sequence to handle complex tasks efficiently. Instead of a single model doing everything, each AI is specialized for a specific function, ensuring better accuracy, efficiency, and interpretability.</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f8f9fa; border-radius: 10px; padding: 20px; margin-bottom: 20px;\">\n",
        "<h3 style=\"color: #0055d4; margin-top: 0; font-size: 18px; margin-bottom: 15px;\">Key Benefits of Multi-LLM Chaining</h3>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "‚úì <strong style=\"color: #0055d4;\">Task Specialization</strong> ‚Äî Each model is optimized for a specific role, leading to higher accuracy\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "‚úì <strong style=\"color: #0055d4;\">Improved Efficiency</strong> ‚Äî Models focus on one task at a time, reducing processing load and response time\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "‚úì <strong style=\"color: #0055d4;\">Scalability</strong> ‚Äî Easily extendable by adding more AI models for deeper analysis\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "‚úì <strong style=\"color: #0055d4;\">Transparency & Interpretability</strong> ‚Äî Step-by-step outputs show AI reasoning, making results easier to trust\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 8px; border-left: 3px solid #0055d4;\">\n",
        "‚úì <strong style=\"color: #0055d4;\">Error Reduction</strong> ‚Äî If an early step detects errors, later models can correct them for a refined output\n",
        "</div>\n",
        "</div>\n",
        "\n",
        "<div style=\"margin-bottom: 20px;\">\n",
        "<h3 style=\"color: #0055d4; font-size: 18px; margin-bottom: 15px;\">Real-World Applications of Multi-LLM Chaining</h3>\n",
        "\n",
        "<div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 10px;\">\n",
        "<div style=\"background: #f0f5ff; padding: 12px; border-radius: 6px; border: 1px solid #e0e0e0;\">\n",
        "<strong style=\"color: #0055d4;\">Legal AI</strong><br>\n",
        "<span style=\"font-size: 14px; color: #666;\">One model extracts case details, another predicts legal outcomes</span>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; padding: 12px; border-radius: 6px; border: 1px solid #e0e0e0;\">\n",
        "<strong style=\"color: #0055d4;\">Customer Support</strong><br>\n",
        "<span style=\"font-size: 14px; color: #666;\">One model classifies sentiment, another generates a response</span>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; padding: 12px; border-radius: 6px; border: 1px solid #e0e0e0;\">\n",
        "<strong style=\"color: #0055d4;\">Content Writing</strong><br>\n",
        "<span style=\"font-size: 14px; color: #666;\">One model creates content, another summarizes or edits it</span>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f0f5ff; padding: 12px; border-radius: 6px; border: 1px solid #e0e0e0;\">\n",
        "<strong style=\"color: #0055d4;\">Fake News Detection</strong><br>\n",
        "<span style=\"font-size: 14px; color: #666;\">One model extracts claims, another fact-checks them</span>\n",
        "</div>\n",
        "</div>\n",
        "\n",
        "<p style=\"margin-top: 15px; font-size: 15px; color: #666;\">By chaining specialized AI models, we can enhance AI reasoning, improve accuracy, and build more intelligent, structured workflows across different industries.</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #e8f5e9; border-radius: 10px; padding: 20px; margin-bottom: 20px; border: 1px solid #4caf50;\">\n",
        "<h3 style=\"color: #2e7d32; margin-top: 0; font-size: 18px; margin-bottom: 15px;\">Example: Medical Diagnosis Chain</h3>\n",
        "<p style=\"font-size: 15px; color: #2e7d32; margin-bottom: 15px;\">In a medical AI pipeline:</p>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 10px; border-left: 3px solid #4caf50;\">\n",
        "<strong style=\"color: #2e7d32;\">1. ChatGPT</strong> ‚Äî Analyzes symptoms and suggests possible conditions\n",
        "</div>\n",
        "\n",
        "<div style=\"background: white; border-radius: 6px; padding: 12px; margin-bottom: 10px; border-left: 3px solid #4caf50;\">\n",
        "<strong style=\"color: #2e7d32;\">2. ChatGPT</strong> ‚Äî Evaluates the list and selects the most likely condition with reasoning\n",
        "</div>\n",
        "\n",
        "<p style=\"margin-top: 15px; font-size: 14px; color: #666;\">This structured approach breaks down decision-making into logical steps, leading to more reliable outputs.</p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "wvyg7LSSgrVi"
      },
      "id": "wvyg7LSSgrVi"
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Step 1: Define the two LLMs\n",
        "\n",
        "# üîπ LLM A ‚Äì generates possible medical conditions\n",
        "llm_medical = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.0)\n",
        "\n",
        "# üîπ LLM B ‚Äì selects the best condition and adds a brief rationale\n",
        "#    (using the newer ‚Äògpt-4o‚Äô release)\n",
        "llm_reasoning = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.0)\n",
        "\n",
        "\n",
        "# ‚úÖ Step 2: Create prompt templates\n",
        "\n",
        "prompt_medical = PromptTemplate(\n",
        "    input_variables=[\"symptoms\"],\n",
        "    template=\"\"\"\n",
        "You are an AI medical assistant. Based on the symptoms provided, suggest up to 3 possible conditions.\n",
        "\n",
        "Symptoms: {symptoms}\n",
        "\n",
        "Respond in a **comma-separated list** (e.g., \"Flu, COVID-19, Pneumonia\").\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "prompt_reasoning = PromptTemplate(\n",
        "    input_variables=[\"conditions\"],\n",
        "    template=\"\"\"\n",
        "You are an AI doctor. Based on the possible conditions listed, pick the **most probable** one and provide a **brief reason**.\n",
        "\n",
        "Possible conditions: {conditions}\n",
        "\n",
        "Respond with **only the best condition + reasoning in ‚â§2 sentences **.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# ‚úÖ Step 3: Build the chain with the `|` operator\n",
        "#    1Ô∏è‚É£ Symptoms ‚ûú LLM A ‚Üí list of conditions\n",
        "#    2Ô∏è‚É£ Lambda wraps that list into {\"conditions\": ‚Ä¶}\n",
        "#    3Ô∏è‚É£ Formatted for LLM B ‚Üí best condition + justification\n",
        "\n",
        "medical_chain = (\n",
        "    prompt_medical\n",
        "    | llm_medical\n",
        "    | (lambda x: {\"conditions\": x.content})\n",
        "    | prompt_reasoning\n",
        "    | llm_reasoning\n",
        ")\n"
      ],
      "metadata": {
        "id": "iIMmsdZFgxXZ"
      },
      "id": "iIMmsdZFgxXZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ Test Case 1: Common Cold Symptoms\n",
        "symptoms_input = \"runny nose, sneezing, mild headache\"\n",
        "\n",
        "response = medical_chain.invoke({\"symptoms\": symptoms_input})\n",
        "\n",
        "print(\"üîç Possible Conditions (GPT-4 Turbo):\", response.content)"
      ],
      "metadata": {
        "id": "MCeLejCTMqOz"
      },
      "id": "MCeLejCTMqOz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ Test Case 2: Severe Flu-like Symptoms\n",
        "symptoms_input = \"fever, chills, muscle pain, fatigue\"\n",
        "\n",
        "response = medical_chain.invoke({\"symptoms\": symptoms_input})\n",
        "\n",
        "print(\"üîç Possible Conditions (GPT-4 Turbo):\", response.content)\n"
      ],
      "metadata": {
        "id": "sip86GtOMwdE"
      },
      "id": "sip86GtOMwdE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîπ Test Case 3: Stomach-related Symptoms\n",
        "symptoms_input = \"nausea, vomiting, diarrhea, stomach cramps\"\n",
        "\n",
        "response = medical_chain.invoke({\"symptoms\": symptoms_input})\n",
        "\n",
        "print(\"üîç Possible Conditions (GPT-4 Turbo):\", response.content)\n"
      ],
      "metadata": {
        "id": "wzJu_UAgMyyC"
      },
      "id": "wzJu_UAgMyyC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úã Hands-On Exercise: Customer Review Analysis & Automated Response"
      ],
      "metadata": {
        "id": "dMppSO10jOgf"
      },
      "id": "dMppSO10jOgf"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ‚úã **Hands-On: Customer Review Sentiment & AI Response**\n",
        "# ==================================================\n",
        "# üìå **Task Instructions:**\n",
        "# 1Ô∏è‚É£ Fill in the missing placeholders (`-----`) to complete the code.\n",
        "# 2Ô∏è‚É£ Ensure both Prompt Templates correctly embed {review} and {sentiment}.\n",
        "# 3Ô∏è‚É£ Run the code and verify the AI-generated sentiment & response.\n",
        "\n",
        "# ‚úÖ Step 1: Import necessary modules\n",
        "\n",
        "# ‚úÖ Step 2: Initialize two different LLMs\n",
        "llm_sentiment = -----(\"gpt-4-turbo\")  # GPT-4 Turbo for sentiment analysis\n",
        "llm_response = -----(\"gpt-4-1106-preview\")  # GPT-4-01 for response generation\n",
        "\n",
        "# ‚úÖ Step 3: Define the first Prompt Template (Sentiment Analysis)\n",
        "sentiment_prompt = -----(\n",
        "    input_variables=[\"-----\"],  # Define the input variable for review input\n",
        "    template=\"\"\"\n",
        "    You are an AI assistant analyzing customer sentiment.\n",
        "\n",
        "    Review: {review}\n",
        "\n",
        "    Respond with either \"Positive\", \"Neutral\", or \"Negative\".\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 4: Define the second Prompt Template (Automated Response)\n",
        "response_prompt = -----(\n",
        "    input_variables=[\"-----\"],  # Define the input variable for sentiment classification\n",
        "    template=\"\"\"\n",
        "    You are an AI customer support agent. Based on the sentiment, generate a short, polite response.\n",
        "\n",
        "    Sentiment: {sentiment}\n",
        "\n",
        "    Response (‚â§ 15 words):\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 5: Create Runnable Chains using the `|` operator\n",
        "sentiment_chain = ----- | -----  # Chain the sentiment prompt with GPT-4 Turbo\n",
        "response_chain = ----- | -----  # Chain the response prompt with GPT-4-01\n",
        "\n",
        "# ‚úÖ Step 6: Run the pipeline\n",
        "\n",
        "# Step 6.1: Sample customer review input\n",
        "customer_review = \"The product is amazing! Great quality and fast delivery. Will buy again.\"\n",
        "\n",
        "# Step 6.2: Analyze sentiment using GPT-4 Turbo\n",
        "sentiment_result = sentiment_chain.-----({\"review\": customer_review})  # Call the function to invoke the model\n",
        "\n",
        "# Step 6.3: Generate AI response using GPT-4-01\n",
        "response_text = response_chain.-----({\"sentiment\": sentiment_result.-----})  # Ensure correct content extraction\n",
        "\n",
        "# ‚úÖ Step 7: Display results\n",
        "print(\"üîç **Detected Sentiment:**\", sentiment_result.-----)  # Extract response content\n",
        "print(\"\\nüí¨ **AI Response:**\", response_text.-----)  # Extract AI-generated response\n",
        "\n",
        "\n",
        "#+++++++ Example Output +++++++#\n",
        "#üîç **Detected Sentiment:** Positive\n",
        "#üí¨ **AI Response:** Thank you for your kind words! We‚Äôre glad you loved it! üòä\n"
      ],
      "metadata": {
        "id": "WV19WC9ujSna"
      },
      "id": "WV19WC9ujSna",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úã Hands-On Exercise: Customer Review Analysis & Automated Response (Merged Chain)"
      ],
      "metadata": {
        "id": "szagChUdO841"
      },
      "id": "szagChUdO841"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# ‚úã **Hands-On: Merged Chain for Customer Review Sentiment & AI Response**\n",
        "# ==================================================\n",
        "# üìå **Task Instructions:**\n",
        "# 1Ô∏è‚É£ Fill in the missing placeholders (`-----`) to complete the chain.\n",
        "# 2Ô∏è‚É£ Ensure sentiment analysis flows correctly into the response generation.\n",
        "# 3Ô∏è‚É£ Run the code and verify AI-generated responses.\n",
        "\n",
        "# ‚úÖ The `PromptTemplate` and LLMs (`llm_sentiment` and `llm_response`) are already defined.\n",
        "\n",
        "# ‚úÖ Step 5: Create a Single Runnable Chain (Merged Approach)\n",
        "full_chain = (\n",
        "    -----  # Step 1: Start with the sentiment prompt\n",
        "    | -----  # Step 2: Pass it to the sentiment analysis LLM\n",
        "    | (lambda x: {\"sentiment\": x.content})  # Step 3: Extract sentiment result\n",
        "    | -----  # Step 4: Format sentiment into response prompt\n",
        "    | -----  # Step 5: Pass it to the response generation LLM\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 6: Run the pipeline\n",
        "\n",
        "# Sample customer review input\n",
        "customer_review = \"The product is amazing! Great quality and fast delivery. Will buy again.\"\n",
        "\n",
        "# Run the full pipeline in one step\n",
        "response = full_chain.invoke({\"review\": customer_review})\n",
        "\n",
        "# ‚úÖ Step 7: Display results\n",
        "print(\"üí¨ **AI Response:**\", response.content)  # Extract AI-generated response\n"
      ],
      "metadata": {
        "id": "RwRSZ76pO5JH"
      },
      "id": "RwRSZ76pO5JH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ AI Agents: Beyond Basic LLM Chaining  \n",
        "\n",
        "## üîπ What are AI Agents?  \n",
        "Unlike simple LLM pipelines where models work in a **fixed sequence**, **AI agents** are **more flexible** and can **make decisions dynamically**. They can interact with **external tools, APIs, memory, and reasoning frameworks** to **adapt their responses** based on the situation.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç **LLM Chaining vs. AI Agents**\n",
        "| Feature           | LLM Chaining                     | AI Agents                      |\n",
        "|------------------|--------------------------------|--------------------------------|\n",
        "| **Execution**    | Fixed sequence of steps        | Dynamic, adaptive behavior    |\n",
        "| **Decision-Making** | Follows predefined logic       | Can reason and choose actions |\n",
        "| **Interactivity** | Limited to internal logic      | Can use APIs, databases, tools |\n",
        "| **Memory**       | No long-term state             | Can remember previous actions |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Why Use AI Agents?**\n",
        "‚úîÔ∏è **Decision-Making** ‚Äì Agents **select actions** dynamically instead of just following a script.  \n",
        "‚úîÔ∏è **Tool Integration** ‚Äì Can access **APIs, databases, and external tools** like search engines or calculators.  \n",
        "‚úîÔ∏è **Memory** ‚Äì Stores past interactions to **improve responses over time**.  \n",
        "‚úîÔ∏è **Multi-Step Reasoning** ‚Äì Can **plan**, execute, and refine responses based on feedback.  \n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Real-World Applications of AI Agents\n",
        "üîπ **Customer Support AI:** Detects user needs, queries databases, and provides **real-time support**.  \n",
        "üîπ **Financial AI:** Analyzes market trends, retrieves **live stock prices**, and recommends investments.  \n",
        "üîπ **Research Assistants:** Searches the web, **extracts insights**, and summarizes articles.  \n",
        "üîπ **Automated Workflow Agents:** Interact with **multiple APIs** to execute complex business tasks.  \n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö° Next Steps: Building an AI Agent  \n",
        "Now, let‚Äôs **create an AI agent** that can **reason, interact with tools, and make decisions** dynamically! üöÄ  \n"
      ],
      "metadata": {
        "id": "sYopzX5PSmNR"
      },
      "id": "sYopzX5PSmNR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßÆ Lab: Building a Math Solver Agent  \n",
        "\n",
        "In this lab, we create an **AI agent** that can solve **math problems dynamically** using a **calculator tool**. The agent **decides** whether to perform calculations itself or use external tools, showcasing **reasoning and tool integration** in LangChain.  \n",
        "\n",
        "The agent is executed using **`.run()`**, which allows it to process user queries and decide on actions dynamically.  \n",
        "\n",
        "---\n",
        "\n",
        "## üîπ What is a Tool in LangChain?  \n",
        "A **tool** in LangChain is an **external function** that an agent can call to perform **specialized tasks** beyond just text generation. Tools help **extend AI capabilities**, allowing it to:  \n",
        "‚úîÔ∏è **Perform calculations** (e.g., a calculator tool)  \n",
        "‚úîÔ∏è **Query APIs** (e.g., fetch stock prices, weather updates)  \n",
        "‚úîÔ∏è **Search databases** (e.g., retrieve company records)  \n",
        "\n",
        "In this lab, we use a **calculator tool** to enable precise **math computation**, ensuring accurate results instead of relying solely on an LLM‚Äôs reasoning.  "
      ],
      "metadata": {
        "id": "s-JOOTTym4Iu"
      },
      "id": "s-JOOTTym4Iu"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üèóÔ∏è Build the Math Solver Agent\n",
        "# ================================\n",
        "# ‚úÖ This cell initializes the agent with a reasoning LLM and a calculator tool.\n",
        "\n",
        "from langchain.agents import initialize_agent, AgentType  # This is used to initialize an agent with a specific type\n",
        "from langchain.tools import Tool  # Tool class is used to define custom tools that the agent can use\n",
        "import operator  # Importing the operator module to perform mathematical and logical operations\n",
        "\n",
        "# ‚úÖ Step 1: Define the LLM\n",
        "llm = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.0)\n",
        "\n",
        "import operator  # Importing the operator module for mathematical operations\n",
        "\n",
        "# ‚úÖ Step 2: Create a Calculator Tool\n",
        "def calculator_tool(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Securely evaluates a mathematical expression and returns the computed result.\n",
        "\n",
        "    This function allows basic arithmetic operations while restricting access to\n",
        "    potentially dangerous built-in functions.\n",
        "\n",
        "    Supported operations:Addition (+), Subtraction (-), Multiplication (*)\n",
        "    - Division (/), Exponentiation (**), Floor Division (//), Modulus (%)\n",
        "\n",
        "    Parameters:\n",
        "    expression (str): A valid mathematical expression in string format\n",
        "                      (e.g., \"5 + 3\", \"10 * 2\", \"8 ** 2\").\n",
        "\n",
        "    Returns:\n",
        "    str: The computed result as a string, or an error message if the input is invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # ‚úÖ Securely evaluate the mathematical expression\n",
        "        # - `eval()` computes the arithmetic operation safely.\n",
        "        # - `{\"__builtins__\": {}}` removes access to all built-in functions, preventing security risks.\n",
        "        # - `operator.__dict__` limits the allowed operations to those defined in the operator module.\n",
        "        result = eval(expression, {\"__builtins__\": {}}, operator.__dict__)\n",
        "\n",
        "        # ‚úÖ Convert the result to a string before returning it\n",
        "        return str(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        # ‚úÖ Handle errors gracefully, such as:\n",
        "        # - Invalid mathematical expressions (e.g., \"five plus three\" instead of \"5 + 3\")\n",
        "        # - Unsupported operations\n",
        "        # - Division by zero\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "calculator = Tool(\n",
        "    name=\"Calculator\",\n",
        "    func=calculator_tool,\n",
        "    description=\"Use this tool to perform basic arithmetic calculations.\"\n",
        ")\n",
        "\n",
        "# ‚úÖ Step 3: Initialize the Math Solver Agent\n",
        "math_solver_agent = initialize_agent(\n",
        "    tools=[calculator],\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "hlWQ9rtoSnLc"
      },
      "id": "hlWQ9rtoSnLc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîπ Why Do We Need an LLM for the Math Solver Agent?  \n",
        "\n",
        "The **LLM** is essential for enabling **natural language understanding and reasoning** in the **Math Solver Agent**.  \n",
        "\n",
        "### **‚úÖ Why Keep the LLM?**  \n",
        "‚úî Interprets **human-like queries** (e.g., `\"Multiply 10 by 3\"` ‚Üí `\"10 * 3\"`)  \n",
        "‚úî Decides **when to use the calculator tool**  \n",
        "‚úî Provides **error handling and explanations** in natural language  \n",
        "‚úî Responds to **unsupported queries** instead of failing  \n",
        "\n",
        "### **‚ùå What Happens Without It?**  \n",
        "üîπ Only strict expressions (e.g., `\"5 + 3\"`) work, no **language understanding**  \n",
        "üîπ The tool **cannot reason** or decide how to handle a query  \n",
        "üîπ The agent **becomes a basic calculator**, losing flexibility  \n",
        "\n",
        "üöÄUsing the LLM makes the agent **more intelligent, flexible, and user-friendly** beyond just executing math operations.  \n"
      ],
      "metadata": {
        "id": "gaySwvBeiTlY"
      },
      "id": "gaySwvBeiTlY"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üß™ Test the Math Solver Agent\n",
        "# ================================\n",
        "# ‚úÖ This cell runs the agent with different math problems.\n",
        "\n",
        "# üîπ Test Cases (Increasing Difficulty):\n",
        "test_cases = [\n",
        "    \"What is 42 * (8 + 3)?\",  # Simple arithmetic\n",
        "    \"Solve for x: 3x + 5 = 20.\",  # Equation solving\n",
        "    \"Integrate (3x^2 + 2x - 5) dx.\",  # Advanced calculus (integration)\n",
        "]\n",
        "\n",
        "# üîπ Run the agent on each test case one by one\n",
        "for query in test_cases:\n",
        "    response = math_solver_agent.invoke(query)\n",
        "    print(f\"üßÆ Query: {query}\")\n",
        "    print(f\"üì¢ Response: {response}\\n\")\n"
      ],
      "metadata": {
        "id": "hOe8c9LemiCs"
      },
      "id": "hOe8c9LemiCs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí° Did You Realize? Your Agent is Smarter Than You Think!\n",
        "\n",
        "### ‚ùì Why does the Math Solver Agent understand `\"Multiply 10 by 3\"` even though there is no prompt?  \n",
        "‚úÖ The agent uses **`ZERO_SHOT_REACT_DESCRIPTION`**, which **automatically prompts the LLM** behind the scenes.  \n",
        "\n",
        "### ‚ùì How does the LLM convert `\"Multiply 10 by 3\"` into `\"10 * 3\"`?  \n",
        "‚úÖ LangChain **asks the LLM to interpret the query** and map words to mathematical symbols before calling the calculator tool.  \n",
        "\n",
        "### ‚ùì What happens if we remove the LLM?  \n",
        "‚úÖ The agent **won‚Äôt understand** `\"Multiply 10 by 3\"`, and only exact expressions like `\"10 * 3\"` will work.  \n",
        "\n",
        "### ‚ùì Does the definition of a tool get passed to the LLM?  \n",
        "‚úÖ Yes! The agent **passes the tool‚Äôs description** to the LLM so it knows when and how to use it.  \n",
        "\n",
        "### ‚ùì Can an agent have multiple tools?  \n",
        "‚úÖ Yes! An agent can use **multiple tools**, and the LLM will decide which one to call based on the query."
      ],
      "metadata": {
        "id": "UXqFE7nxjjlY"
      },
      "id": "UXqFE7nxjjlY"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# ‚úã Hands-On: Build a Unit Conversion Agent\n",
        "# ================================\n",
        "# üìå **Task Instructions:**\n",
        "# 1Ô∏è‚É£ Fill in the missing placeholders (`-----`) to complete the code.\n",
        "# 2Ô∏è‚É£ Ensure the tool and agent are correctly initialized.\n",
        "# 3Ô∏è‚É£ Test the agent by providing a conversion query.\n",
        "\n",
        "from langchain.----- import initialize_agent, AgentType  # For agent initialization\n",
        "from langchain.----- import Tool  # Tool class to define custom tools for the agent\n",
        "\n",
        "# ================================\n",
        "# ‚úÖ Step 1: Define the LLM\n",
        "# ================================\n",
        "# üîπ Fill in the placeholder to define the LLM\n",
        "llm = -----(\"gpt-4-turbo\", temperature=0.0)\n",
        "\n",
        "# ================================\n",
        "# ‚úÖ Step 2: Create a Unit Conversion Tool\n",
        "# ================================\n",
        "def unit_conversion_tool(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Handles unit conversions for basic metrics like length, weight, and temperature.\n",
        "\n",
        "    Supported conversions:\n",
        "    - Miles to Kilometers\n",
        "    - Kilograms to Pounds\n",
        "    - Celsius to Fahrenheit\n",
        "\n",
        "    The function extracts the numerical value from the query string, performs the\n",
        "    specified conversion, and returns the result as a formatted string.\n",
        "\n",
        "    Returns:\n",
        "    str: The conversion result or an error message if the query is unsupported or invalid.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if \"miles to kilometers\" in query.lower():\n",
        "            miles = float(query.split()[0])\n",
        "            return f\"{miles} miles is {miles * 1.60934:.2f} kilometers.\"\n",
        "        elif \"kilograms to pounds\" in query.lower():\n",
        "            kg = float(query.split()[0])\n",
        "            return f\"{kg} kilograms is {kg * 2.20462:.2f} pounds.\"\n",
        "        elif \"celsius to fahrenheit\" in query.lower():\n",
        "            celsius = float(query.split()[0])\n",
        "            return f\"{celsius}¬∞C is {celsius * 9/5 + 32:.2f}¬∞F.\"\n",
        "        else:\n",
        "            return (\n",
        "                \"Unsupported conversion. Supported conversions:\\n\"\n",
        "                \"- Miles to Kilometers\\n\"\n",
        "                \"- Kilograms to Pounds\\n\"\n",
        "                \"- Celsius to Fahrenheit\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        return f\"Error: Unable to process the query. Details: {str(e)}\"\n",
        "\n",
        "\n",
        "# üîπ Fill in the placeholder to define the tool\n",
        "unit_converter = Tool(\n",
        "    name=\"Unit Converter\",\n",
        "    func=-----,  # Function to handle unit conversions\n",
        "    description=\"Use this tool to convert units like miles to kilometers, kilograms to pounds, and Celsius to Fahrenheit.\"\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# ‚úÖ Step 3: Initialize the Unit Conversion Agent\n",
        "# ================================\n",
        "# üîπ Fill in the placeholder to initialize the agent\n",
        "unit_conversion_agent = initialize_agent(\n",
        "    tools=[-----],  # Tool for unit conversion\n",
        "    llm=llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# ‚úÖ Step 4: Test the Unit Conversion Agent\n",
        "# ================================\n",
        "# üîπ Example query for conversion\n",
        "query = \"5 miles to kilometers\"\n",
        "\n",
        "# üîπ Run the agent with the query\n",
        "response = unit_conversion_agent.run(query)\n",
        "\n",
        "# üîπ Print the response\n",
        "print(\"üîÑ Unit Conversion Agent Response:\\n\", response)\n"
      ],
      "metadata": {
        "id": "P_YFRx0LxisN"
      },
      "id": "P_YFRx0LxisN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- üìä MarketMaster Agent Overview -->\n",
        "\n",
        "<h2 style=\"color: #2c3e50;\">üìä MarketMaster Agent: Smart Stock Analytics & Advice</h2>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<p>\n",
        "This AI-powered agent analyzes <strong>AAPL (Apple Inc.) stock data</strong> using modern LangChain agents with OpenAI's GPT-4. It supports reasoning, tool invocation, and investment insight generation.\n",
        "</p>\n",
        "\n",
        "---\n",
        "\n",
        "### üîß Features\n",
        "\n",
        "<ul>\n",
        "  <li><strong>üìà Moving Average Calculator:</strong> Computes short-term stock trends (e.g., 7-day moving average).</li>\n",
        "  <li><strong>üí° Investment Advisor:</strong> Uses price movements to suggest <em>buy, sell, or hold</em> decisions.</li>\n",
        "  <li><strong>üß† LangChain ReAct Agent:</strong> Chooses which tool to use, executes it, and delivers a final answer.</li>\n",
        "</ul>\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è How It Works\n",
        "\n",
        "<ol>\n",
        "  <li>Historical AAPL data is loaded via a Dropbox CSV link.</li>\n",
        "  <li>Two tools are registered using LangChain's <code>Tool</code> class:\n",
        "    <ul>\n",
        "      <li><strong>Moving Average Tool</strong>: Parses user input and calculates the average price over N days.</li>\n",
        "      <li><strong>Investment Advice Tool</strong>: Analyzes latest price delta and invokes the LLM to generate advice.</li>\n",
        "    </ul>\n",
        "  </li>\n",
        "  <li>An <strong>LLM-based ReAct Agent</strong> reasons through each query and chooses the appropriate tool.</li>\n",
        "</ol>\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Example Queries\n",
        "\n",
        "```python\n",
        "\"Calculate the 7-day moving average for AAPL.\"\n",
        "\"Should I invest in AAPL?\"\n",
        "\"What is the square root of 16?\"\n"
      ],
      "metadata": {
        "id": "BaXtG9OwxsTp"
      },
      "id": "BaXtG9OwxsTp"
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# üì¶ Imports: Libraries & LangChain Components\n",
        "# ========================\n",
        "import pandas as pd\n",
        "from langchain.agents import create_react_agent, AgentExecutor\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.tools import Tool\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# ====================================================\n",
        "# üìä MarketMaster Agent: Smart Stock Analytics & Advice\n",
        "# ====================================================\n",
        "# ‚úÖ Powered by: OpenAI GPT-4 + LangChain's ReAct Agent (Modern)\n",
        "# ----------------------------------------------------\n",
        "# - Calculates Moving Averages from Historical AAPL Data\n",
        "# - Provides AI-Generated Buy/Sell/Hold Investment Advice\n",
        "# - Uses ReAct Agent for Reasoning + Tool Calling\n",
        "\n",
        "# ========================\n",
        "# üì• Load Stock Data (AAPL)\n",
        "# ========================\n",
        "csv_url = \"https://www.dropbox.com/scl/fi/ysqxvj39gx2bkl4husg3y/HistoricalData_1739205046441.csv?rlkey=36q4rjpvvmwt9fyf3fftxwvb3&dl=1\"\n",
        "stock_data = pd.read_csv(csv_url)\n",
        "# Process and clean up the Close/Last column for numeric calculation\n",
        "stock_data['Close/Last'] = stock_data['Close/Last'].str.replace('$', '').astype(float)\n",
        "\n",
        "# ========================\n",
        "# üß† Define LLMs for Reasoning & Advice\n",
        "# ========================\n",
        "llm_analysis = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.0)     # For reasoning about which tools to use\n",
        "llm_investment = ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.0)   # For concise investment advice\n",
        "\n",
        "# ========================\n",
        "# üõ†Ô∏è Tool 1: Moving Average Calculator\n",
        "# ========================\n",
        "# This tool parses a user's query to extract a time window (e.g., \"7-day\")\n",
        "# and calculates the moving average of AAPL for that period.\n",
        "def calculate_moving_average(query: str) -> str:\n",
        "    try:\n",
        "        words = query.lower().split()\n",
        "        for word in words:\n",
        "            if \"day\" in word:\n",
        "                days_str = \"\".join(filter(str.isdigit, word))\n",
        "                if days_str:\n",
        "                    days = int(days_str)\n",
        "                    break\n",
        "        else:\n",
        "            # If no \"day\" found, try to interpret the whole query as a number of days\n",
        "            try:\n",
        "                days = int(query.strip())\n",
        "            except ValueError:\n",
        "                return \"Error: Could not determine the number of days for the moving average.\"\n",
        "\n",
        "        if len(stock_data) < days:\n",
        "            return f\"Error: Not enough data to calculate a {days}-day moving average. Available data points: {len(stock_data)}\"\n",
        "        if days <= 0:\n",
        "            return \"Error: Number of days must be positive.\"\n",
        "\n",
        "        moving_avg = stock_data['Close/Last'].tail(days).mean()\n",
        "        return f\"The {days}-day moving average for AAPL is ${moving_avg:.2f}.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in calculate_moving_average: {str(e)}\"\n",
        "\n",
        "moving_avg_tool = Tool(\n",
        "    name=\"Moving Average Calculator\",\n",
        "    func=calculate_moving_average,\n",
        "    description=\"Calculates the moving average for the last N days based on AAPL closing prices. Input should be a number or a phrase like '7-day'.\"\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# üõ†Ô∏è Tool 2: Investment Advice Generator\n",
        "# ========================\n",
        "# This tool uses recent AAPL price movement to suggest a \"buy\", \"sell\", or \"hold\" action.\n",
        "def generate_investment_advice(query: str) -> str:\n",
        "    try:\n",
        "        # Ensure there are at least two data points to calculate change\n",
        "        if len(stock_data) < 2:\n",
        "            return \"Error: Not enough data to generate investment advice.\"\n",
        "\n",
        "        latest_entry = stock_data.iloc[-1]\n",
        "        previous_entry = stock_data.iloc[-2]\n",
        "        price_change = latest_entry['Close/Last'] - previous_entry['Close/Last']\n",
        "        # Calculate percent change, avoid division by zero\n",
        "        price_change_percent = (price_change / previous_entry['Close/Last']) * 100 if previous_entry['Close/Last'] != 0 else 0\n",
        "        trend = \"upward üìà\" if price_change > 0 else (\"downward üìâ\" if price_change < 0 else \"stable\")\n",
        "\n",
        "        # Use a prompt template to provide structured data to the LLM\n",
        "        advice_prompt_template = PromptTemplate.from_template(\n",
        "            \"\"\"\n",
        "            Analyze the following AAPL stock data and provide a brief investment recommendation (buy, sell, or hold) based *only* on this data.\n",
        "\n",
        "            AAPL Stock Data:\n",
        "            - Latest Close Price: {latest_price}\n",
        "            - Price Change (vs previous day): {price_change:.2f} ({price_change_percent:.2f}%)\n",
        "            - Short-term Trend (based on price change): {trend}\n",
        "\n",
        "            Investment Recommendation (buy, sell, or hold, max 15 words):\n",
        "            \"\"\"\n",
        "        )\n",
        "        advice_prompt = advice_prompt_template.format(\n",
        "            latest_price=latest_entry['Close/Last'],\n",
        "            price_change=price_change,\n",
        "            price_change_percent=price_change_percent,\n",
        "            trend=trend\n",
        "        )\n",
        "\n",
        "        return llm_investment.invoke(advice_prompt).content\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error in generate_investment_advice: {str(e)}\"\n",
        "\n",
        "investment_tool = Tool(\n",
        "    name=\"Investment Advice Generator\",\n",
        "    func=generate_investment_advice,\n",
        "    description=\"Analyzes AAPL's latest trend and provides a brief buy/sell/hold investment advice. Use when asked for investment advice.\"\n",
        ")\n",
        "\n",
        "# ========================\n",
        "# ü§ñ Agent Initialization (Modern API)\n",
        "# ========================\n",
        "\n",
        "# Prepare a custom prompt template for the ReAct agent style (Reason+Act).\n",
        "# The prompt gives the agent step-by-step instructions and defines the output structure.\n",
        "agent_prompt = PromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    You are MarketMaster, a smart stock analysis assistant.\n",
        "    You have access to the following tools:\n",
        "\n",
        "    {tools}\n",
        "\n",
        "    Use the tools provided to answer questions about AAPL stock trends and give investment advice.\n",
        "    The user will give you a question about AAPL stock.\n",
        "\n",
        "    Follow this format:\n",
        "\n",
        "    Question: the input question you must answer\n",
        "    Thought: you should always think about what to do\n",
        "    Action: the action to take, should be one of [{tool_names}]\n",
        "    Action Input: the input to the action\n",
        "    Observation: the result of the action\n",
        "    ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "    Thought: I now know the final answer\n",
        "    Final Answer: the final answer to the original input question\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Question: {input}\n",
        "    Thought:{agent_scratchpad}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# üß† Create the agent: This agent will reason through the prompt and decide which tool to use.\n",
        "agent = create_react_agent(\n",
        "    llm=llm_analysis,\n",
        "    tools=[moving_avg_tool, investment_tool],\n",
        "    prompt=agent_prompt\n",
        ")\n",
        "\n",
        "# üö¶ AgentExecutor: The main controller that manages the agent's workflow.\n",
        "# It receives the user's query, lets the agent choose tools/actions step by step,\n",
        "# executes them, tracks intermediate reasoning (\"Thoughts\" and \"Actions\"),\n",
        "# and finally returns a complete answer.\n",
        "marketmaster_agent = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=[moving_avg_tool, investment_tool],\n",
        "    verbose=True,                # Show intermediate reasoning steps for transparency/debugging\n",
        "    handle_parsing_errors=True   # Make execution more robust to unexpected agent outputs\n",
        ")\n"
      ],
      "metadata": {
        "id": "-VXDJT4zoSSD"
      },
      "id": "-VXDJT4zoSSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# üöÄ Run Example Queries\n",
        "# ========================\n",
        "query1 = \"Calculate the 7-day moving average for AAPL.\"\n",
        "query2 = \"Should I invest in AAPL?\"\n",
        "query3 = \"What is the square root of 16?\" # Example of a query the agent can't handle with its tools\n",
        "\n",
        "test_queries = [query1, query2, query3]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"--- Processing Query: {query} ---\")\n",
        "    try:\n",
        "        response = marketmaster_agent.invoke({\"input\": query})\n",
        "        print(f\"üì¢ Response for '{query}': {response['output']}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred while processing '{query}': {e}\\n\")"
      ],
      "metadata": {
        "id": "yB7a2dfiUpHc"
      },
      "id": "yB7a2dfiUpHc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ### üü¶ SELF-STUDY ACTIVITY\n",
        ">\n",
        "> #### üìà Chaining Analytics and Graphing Agents\n",
        ">\n",
        "> This activity is for self-study. It demonstrates how to **combine two AI agents** in sequence:\n",
        ">\n",
        "> 1. **üîç Analytics Agent** (MarketMaster): Answers questions using tools like moving averages and investment advice.\n",
        "> 2. **üìä Graphing Agent**: Generates a 10-day AAPL stock trend chart using `matplotlib`.\n",
        ">\n",
        "> ---\n",
        ">\n",
        "> #### üõ†Ô∏è How It Works\n",
        ">\n",
        "> - The `analytics_agent` is reused from your existing MarketMaster.\n",
        "> - A `graph_tool` is defined to create and save a plot of AAPL‚Äôs last 10 days.\n",
        "> - The `graphing_agent` is initialized with this tool and chained after the analytics run.\n",
        ">\n",
        "> ---\n",
        ">\n",
        "> #### ‚úÖ Example Flow\n",
        ">\n",
        "> ```python\n",
        "> analytics_response = analytics_agent.invoke({\"input\": \"Provide the latest stock data for AAPL.\"})\n",
        "> graph_response = graphing_agent.invoke({\"input\": \"Generate a graph for AAPL stock trend.\"})\n",
        "> ```\n",
        ">\n",
        "> _This part is for self-study and hands-on exploration._\n"
      ],
      "metadata": {
        "id": "Jah4R3AugF7q"
      },
      "id": "Jah4R3AugF7q"
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# üìà Chaining Analytics and Graphing Agents\n",
        "# ================================\n",
        "# ‚úÖ This cell demonstrates chaining two agents: one for analytics and another for graphing.\n",
        "\n",
        "import matplotlib.pyplot as plt  # For graphing stock price trends\n",
        "from langchain.tools import Tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "# ================================\n",
        "# ‚úÖ Step 1: Define the Graphing Tool\n",
        "# ================================\n",
        "def generate_stock_graph(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Generates a stock price trend graph for AAPL over the last 10 days.\n",
        "    \"\"\"\n",
        "    # Extract the last 10 days of data\n",
        "    recent_data = stock_data.tail(10)\n",
        "    dates = recent_data['Date']\n",
        "    close_prices = recent_data['Close/Last']\n",
        "\n",
        "    # Create the graph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(dates, close_prices, marker='o', linestyle='-', color='blue')\n",
        "    plt.title(\"AAPL Stock Price Trend (Last 10 Days)\", fontsize=14)\n",
        "    plt.xlabel(\"Date\", fontsize=12)\n",
        "    plt.ylabel(\"Close Price ($)\", fontsize=12)\n",
        "    plt.xticks(rotation=45, fontsize=10)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"aapl_stock_trend.png\")  # Save the graph as an image\n",
        "    plt.close()\n",
        "\n",
        "    return \"üìä Stock trend graph generated successfully: 'aapl_stock_trend.png'\"\n",
        "\n",
        "# üîπ Register the graphing tool\n",
        "graph_tool = Tool(\n",
        "    name=\"Graph Generator\",\n",
        "    func=generate_stock_graph,\n",
        "    description=\"Generates a graph of AAPL's stock price trend over the last 10 days.\"\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# ‚úÖ Step 2: Chain the Agents\n",
        "# ================================\n",
        "\n",
        "# üîπ Analytics Agent (from previous step)\n",
        "# Correct the variable name from 'stock_agent' to 'marketmaster_agent'\n",
        "analytics_agent = marketmaster_agent  # This uses the existing analytics agent\n",
        "\n",
        "# üîπ Graphing Agent\n",
        "llm_graph = ChatOpenAI(model_name=\"gpt-4-turbo\", temperature=0.0)\n",
        "graphing_agent = initialize_agent(\n",
        "    tools=[graph_tool],  # The graphing tool generates the stock graph\n",
        "    llm=llm_graph,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# üîπ Chain Execution: Run analytics first, then graphing\n",
        "query = \"Provide the latest stock data for AAPL.\"\n",
        "\n",
        "# Step 1: Run the analytics agent\n",
        "# Use invoke() instead of run() and access the output key 'output'\n",
        "analytics_response = analytics_agent.invoke({\"input\": query})\n",
        "print(\"üìà Analytics Agent Response:\\n\", analytics_response['output'])\n",
        "\n",
        "# Step 2: Run the graphing agent\n",
        "# Use invoke() instead of run() and access the output key 'output'\n",
        "graph_response = graphing_agent.invoke({\"input\": \"Generate a graph for AAPL stock trend.\"})\n",
        "print(\"\\nüìä Graphing Agent Response:\\n\", graph_response['output'])"
      ],
      "metadata": {
        "id": "ZxaXayThu8L8"
      },
      "id": "ZxaXayThu8L8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GGNagCch-bvZ"
      },
      "id": "GGNagCch-bvZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr>\n",
        "\n",
        "<h2 style=\"color:#2e7d32;\">üéâ Congratulations! You‚Äôve Completed the Lab</h2>\n",
        "\n",
        "<p>\n",
        "In this lab, you successfully learned how to:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>‚úÖ Chain multiple agents together in LangChain</li>\n",
        "  <li>‚úÖ Use <strong>pipe operators</strong> and structured workflows to pass information across tools</li>\n",
        "  <li>‚úÖ Develop intelligent agents and <strong>equip them with tools</strong> for analytics, reasoning, and graphing</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "By combining <code>ReAct-style agents</code> with real-world tools like Pandas and Matplotlib, you've built a powerful foundation for building AI assistants that can reason and act.\n",
        "</p>\n",
        "\n",
        "<p style=\"color:#1565c0;\"><strong>Enjoy your learning journey!</strong> Keep exploring, building, and pushing the boundaries of what agents can do.</p>\n"
      ],
      "metadata": {
        "id": "L-RXFahMgXw8"
      },
      "id": "L-RXFahMgXw8"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8hDBK3uxgYC-"
      },
      "id": "8hDBK3uxgYC-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}