{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamsheer-tech18/AppliedGenAI/blob/main/M3_Lab1_Prompting_Strategies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Intro Section -->\n",
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 30px; border-radius: 12px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.1);\">\n",
        "    <h1 style=\"margin-bottom: 10px; font-size: 32px;\">Introduction to Prompting Strategies</h1>\n",
        "    <p style=\"font-size: 18px; margin: 0;\">Instructor: <strong>Dr. Dehghani</strong></p>\n",
        "</div>\n",
        "\n",
        "<!-- Spacer -->\n",
        "<div style=\"height: 30px;\"></div>\n",
        "\n",
        "<!-- Why It Matters Section -->\n",
        "<div style=\"background: #ffffff; padding: 25px; border-radius: 10px; border-left: 6px solid #0055d4; box-shadow: 0 4px 8px rgba(0,0,0,0.05);\">\n",
        "    <h2 style=\"margin-top: 0; color: #001a70;\">Why Prompting Strategies Matter</h2>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Imagine you‚Äôre working with a junior engineer. You say:  \n",
        "        <em>‚ÄúOptimize the system.‚Äù</em><br>\n",
        "        They‚Äôll probably ask: <em>‚ÄúWhich system? Optimize for cost, speed, or energy? Any constraints?‚Äù</em> üßê\n",
        "    </p>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Now try this instead:  \n",
        "        <em>‚ÄúAnalyze the HVAC system and minimize energy consumption while keeping temperatures between 22-24¬∞C. Provide a cost breakdown.‚Äù</em>  \n",
        "    </p>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        That‚Äôs not just a prompt‚Äîit‚Äôs a <strong>clear strategy</strong> with defined objectives and boundaries.\n",
        "        And that‚Äôs exactly what AI models need to perform at their best.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<!-- Tip Section -->\n",
        "<div style=\"background: #f5faff; padding: 20px; border-radius: 8px; border-left: 5px solid #0055d4; margin-top: 30px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #0055d4;\">üí° Pro Tip</h3>\n",
        "    <p style=\"margin: 0; font-size: 16px; line-height: 1.6;\">\n",
        "        AI models appreciate well-structured instructions just like engineers appreciate complete design specs.\n",
        "        Be specific, set clear goals, and watch the results improve!\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<!-- Upcoming Topics -->\n",
        "<div style=\"margin-top: 40px; text-align: center;\">\n",
        "    <h3 style=\"color: #001a70;\">What‚Äôs Ahead</h3>\n",
        "    <ul style=\"list-style: none; padding: 0; font-size: 16px; line-height: 1.8;\">\n",
        "        <li>üìö Basic Prompting Types</li>\n",
        "        <li>üß© Advanced Strategies</li>\n",
        "        <li>üìä Application-Specific Techniques</li>\n",
        "    </ul>\n",
        "    <p style=\"font-size: 16px; color: #333;\">Let‚Äôs engineer some powerful AI conversations! üõ†Ô∏è</p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "VuSW9V7pZvnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Section Header -->\n",
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; border-radius: 12px; text-align: center; box-shadow: 0 4px 12px rgba(0,0,0,0.1);\">\n",
        "    <h1 style=\"margin-bottom: 10px; font-size: 30px;\">üìö Basic Prompting Types</h1>\n",
        "</div>\n",
        "\n",
        "<!-- Spacer -->\n",
        "<div style=\"height: 25px;\"></div>\n",
        "\n",
        "<!-- Zero-Shot Prompting -->\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4; margin-bottom: 20px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #001a70;\">1Ô∏è‚É£ Zero-Shot Prompting</h3>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Provide only the task without any examples.  \n",
        "        <strong>Use When:</strong> The task is simple and well-known by the model.  \n",
        "        <em>Example:</em> ‚ÄúTranslate 'Hello' to French.‚Äù\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "cjZ5fTTNbfyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Set Up LLM and OpenAI API\n",
        "# ==========================\n",
        "# Import required libraries\n",
        "from google.colab import userdata\n",
        "import openai\n",
        "import os\n",
        "\n",
        "# Load the OpenAI API key securely from Colab secrets\n",
        "api_key = userdata.get('OpenAI_API_Key')\n",
        "\n",
        "# Check that the API key was found\n",
        "if api_key is None:\n",
        "    raise ValueError(\"‚ùå API Key not found. Please store your OpenAI API key using Colab secrets.\")\n",
        "\n",
        "# Set API key as environment variable for OpenAI\n",
        "os.environ[\"OpenAI_API_Key\"] = api_key\n",
        "\n",
        "# Initialize OpenAI client\n",
        "client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "print(\"‚úÖ OpenAI API Key successfully loaded and environment is ready!\")\n",
        "\n",
        "# ==========================\n",
        "# üìå Set LLM Model to GPT-3.5\n",
        "# ==========================\n",
        "# Define which LLM model to use\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "print(f\"‚úÖ LLM model set to: {model_name}\")\n"
      ],
      "metadata": {
        "id": "dB3jBmICZwED",
        "outputId": "d811b711-40fb-446d-f76a-dc63a4a3df21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ OpenAI API Key successfully loaded and environment is ready!\n",
            "‚úÖ LLM model set to: gpt-3.5-turbo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Zero-Shot Test: Hidden Formula Sequence\n",
        "# ==========================\n",
        "\n",
        "hard_sequence_prompt_zero = (\n",
        "    \"The sequence is: 3, 12, 27, 48, 75, ___. What‚Äôs next?\"\n",
        ")\n",
        "\n",
        "response_zero_hard = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": hard_sequence_prompt_zero}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "print(\"üîπ LLM Response (Zero-Shot - Hard Sequence):\\n\")\n",
        "print(response_zero_hard.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "id": "AO4UhSJxb0Uv",
        "outputId": "c6c0d57a-288a-48da-aefd-73c77eb4068a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ LLM Response (Zero-Shot - Hard Sequence):\n",
            "\n",
            "The pattern in the sequence is adding consecutive odd numbers to the previous number. \n",
            "\n",
            "3 + 9 = 12\n",
            "12 + 15 = 27\n",
            "27 + 21 = 48\n",
            "48 + 27 = 75\n",
            "\n",
            "Therefore, the next number in the sequence would be 75 + 33 = 108. \n",
            "\n",
            "So, the next number in the sequence is 108.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<!-- One-Shot Prompting -->\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4; margin-bottom: 20px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #001a70;\">2Ô∏è‚É£ One-Shot Prompting</h3>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Provide one clear example along with the instruction.  \n",
        "        <strong>Use When:</strong> You want to guide the model‚Äôs behavior with a single example.  \n",
        "        <em>Example:</em> ‚ÄúTranslate 'Hello' to French: Bonjour. Now translate 'Goodbye'.‚Äù\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "zN2CjeJunlV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Zero-Shot vs One-Shot Comparison: Alternating Pattern Sequence (Correct One-Shot)\n",
        "# ==========================\n",
        "\n",
        "model_name = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Zero-Shot Prompt (No Example)\n",
        "zero_shot_prompt = (\n",
        "    \"The sequence is: 1, 4, 2, 9, 3, 16, 4, ___. What number should replace the blank?\"\n",
        ")\n",
        "\n",
        "# One-Shot Prompt (One Example + New Question)\n",
        "one_shot_prompt = (\n",
        "    \"Example:\\n\"\n",
        "    \"The sequence is: 1, 1, 2, 4, 3, 9, ___. What‚Äôs next?\\n\"\n",
        "    \"Answer: 4.\\n\\n\"\n",
        "    \"Now solve this one:\\n\"\n",
        "    \"The sequence is: 1, 4, 2, 9, 3, 16, 4, ___. What number should replace the blank?\"\n",
        ")\n",
        "\n",
        "# Run Zero-Shot\n",
        "response_zero = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Run One-Shot\n",
        "response_one = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": one_shot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Display Results\n",
        "print(\"üîπ Zero-Shot Response:\\n\" + \"-\"*40)\n",
        "print(response_zero.choices[0].message.content.strip())\n",
        "\n",
        "print(\"\\n\\nüîπ One-Shot Response:\\n\" + \"-\"*40)\n",
        "print(response_one.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "id": "AWI0m9_eciJK",
        "outputId": "f914761f-8608-4f5e-fd30-6bd1dcec54b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Zero-Shot Response:\n",
            "----------------------------------------\n",
            "The pattern is: \n",
            "\n",
            "1^2 = 1\n",
            "2^2 = 4\n",
            "3^2 = 9\n",
            "4^2 = 16\n",
            "\n",
            "Therefore, the next number in the sequence should be 5^2 = 25. \n",
            "\n",
            "So, the number that should replace the blank is 25.\n",
            "\n",
            "\n",
            "üîπ One-Shot Response:\n",
            "----------------------------------------\n",
            "Answer: 25.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<!-- Few-Shot Prompting -->\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4; margin-bottom: 20px;\">\n",
        "    <h3 style=\"margin-top: 0; color: #001a70;\">3Ô∏è‚É£ Few-Shot Prompting</h3>\n",
        "    <p style=\"font-size: 16px; line-height: 1.6;\">\n",
        "        Provide multiple examples to clearly demonstrate the pattern.  \n",
        "        <strong>Use When:</strong> The task is complex or requires understanding a specific format.  \n",
        "        <em>Example:</em>  \n",
        "        - ‚ÄúTranslate 'Hello' to French: Bonjour.‚Äù  \n",
        "        - ‚ÄúTranslate 'Goodbye' to French: Au revoir.‚Äù  \n",
        "        - ‚ÄúTranslate 'Thank you' to French: Merci.‚Äù  \n",
        "        Now translate 'Good night'.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<!-- Spacer -->\n",
        "<div style=\"height: 30px;\"></div>\n",
        "\n",
        "<!-- Closing Tip -->\n",
        "<div style=\"background: #f5faff; padding: 20px; border-radius: 8px; border-left: 5px solid #0055d4;\">\n",
        "    <h3 style=\"margin-top: 0; color: #0055d4;\">üí° Quick Reminder</h3>\n",
        "    <p style=\"margin: 0; font-size: 16px; line-height: 1.6;\">\n",
        "        The more complex the task, the more examples you should provide. But remember, too many examples can make prompts bulky and inefficient.\n",
        "    </p>\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "DdXDGKuAnawA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Few-Shot Prompting Example: Ultra-Hard Pattern (3 Hidden Rules)\n",
        "# ==========================\n",
        "\n",
        "model_name = \"gpt-4-turbo\"  # Best for complex reasoning\n",
        "\n",
        "# Few-Shot Prompt with 2 Examples\n",
        "few_shot_prompt = (\n",
        "    \"Example 1:\\n\"\n",
        "    \"The sequence is: 1, 1, 2, 4, 3, 9, ___. What‚Äôs next?\\n\"\n",
        "    \"Answer: 4.\\n\\n\"\n",
        "    \"Example 2:\\n\"\n",
        "    \"The sequence is: 1, 1, 2, 4, 4, 9, 7, 16, ___. What‚Äôs next?\\n\"\n",
        "    \"Answer: 11.\\n\\n\"\n",
        "    \"Now try this one:\\n\"\n",
        "    \"The sequence is: 1, 1, 2, 4, 4, 9, 7, 16, 11, ___, 16, 36. What number should replace the blank?\"\n",
        ")\n",
        "\n",
        "# Run Few-Shot Prompt\n",
        "response_few = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": few_shot_prompt}],\n",
        "    temperature=0.5\n",
        ")\n",
        "\n",
        "# Display Result\n",
        "print(\"üîπ Few-Shot Prompting (Two Examples Provided):\")\n",
        "print(\"-\" * 40)\n",
        "print(response_few.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "id": "3FLfQaF8mwB9",
        "outputId": "c715a6ff-7da9-4cb3-ac3f-8cca66917100",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Few-Shot Prompting (Two Examples Provided):\n",
            "----------------------------------------\n",
            "To solve this sequence, let's analyze the given numbers and see if we can identify a pattern or rule:\n",
            "\n",
            "The sequence is: 1, 1, 2, 4, 4, 9, 7, 16, 11, ___, 16, 36.\n",
            "\n",
            "Let's look at each number and see if there's a relationship between them:\n",
            "\n",
            "1. The first few numbers don't provide a clear pattern, so let's look at the later numbers:\n",
            "   - 4 (squared) = 16\n",
            "   - 9 (squared) = 81, but we see 9 and then 7, which doesn't directly relate.\n",
            "   - 4 (squared) = 16 (again)\n",
            "   - 16 (squared) = 256, but we see 16 and then 36 (which is 6 squared).\n",
            "\n",
            "2. We can observe that every few numbers, the sequence involves squares:\n",
            "   - 4, 4, 9, 7, 16, 11, ___, 16, 36\n",
            "   - Squares observed: 4, 9, 16, 36 (4^2, 3^2, 4^2, 6^2)\n",
            "\n",
            "3. Let's see if there's a pattern in the numbers immediately before the squares:\n",
            "   - Before 4 (which is 2^2), there is 2.\n",
            "   - Before 9 (which is 3^2), there is 4.\n",
            "   - Before 16 (which is 4^2), there is 7.\n",
            "   - Before 36 (which is 6^2), there should be 11 (as 5^2 = 25 doesn't fit, but we see a pattern of increasing odd numbers: 4, 7, 11).\n",
            "\n",
            "4. The pattern between the non-square numbers seems to be increasing by 3, then by 4:\n",
            "   - 2 to 4 (increase by 2)\n",
            "   - 4 to 7 (increase by 3)\n",
            "   - 7 to 11 (increase by 4)\n",
            "\n",
            "5. Following this pattern, after 11, the next increase should be by 5:\n",
            "   - 11 + 5 = 16\n",
            "\n",
            "Thus, the number that should replace the blank in the sequence 1, 1, 2, 4, 4, 9, 7, 16, 11, ___, 16, 36 is 16.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "m0ICbL-NnaGw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Advanced Prompting Techniques  \n",
        "\n",
        "Moving beyond basic prompting methods like zero-shot and few-shot, advanced strategies help enhance the reasoning and adaptability of large language models (LLMs). These techniques guide the model's thought process to handle complex tasks more effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### üîó Chain-of-Thought (CoT) Prompting  \n",
        "\n",
        "Chain-of-Thought prompting encourages models to **explain their intermediate reasoning steps**, leading to more transparent and accurate conclusions. By structuring prompts to include logical steps, CoT improves the model‚Äôs ability to solve complex reasoning tasks.\n",
        "\n",
        "**Why is CoT Important?**  \n",
        "- ‚úîÔ∏è Improves performance on multi-step reasoning tasks.  \n",
        "- ‚úîÔ∏è Helps produce logically structured and coherent responses.  \n",
        "- ‚úîÔ∏è Breaks down complex problems into manageable steps.\n",
        "\n",
        "üìñ **Reference:** [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)\n",
        "\n",
        "---\n",
        "\n",
        "*Next, explore practical examples of Chain-of-Thought prompting.*\n"
      ],
      "metadata": {
        "id": "cJYU0rOaTD5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Chain-of-Thought Demonstration: Make 110 with Five 5's\n",
        "# ==========================\n",
        "\n",
        "model_name = \"gpt-4-turbo\"\n",
        "\n",
        "# Zero-Shot Prompt (No Reasoning Encouraged)\n",
        "zero_shot_prompt = (\n",
        "    \"Use exactly five 5‚Äôs and only four operations (+, -, *, /) and parentheses to make 110.\"\n",
        ")\n",
        "\n",
        "# Chain-of-Thought Prompt (Encourages Step-by-Step Reasoning)\n",
        "cot_prompt = (\n",
        "    \"Let's solve this step by step.\\n\"\n",
        "    \"We need to use exactly five 5‚Äôs and only four operations (+, -, *, /) and parentheses to make 110.\\n\"\n",
        "    \"Step 1: Assume we cannot combine the 5's to form larger numbers (e.g., 55).\\n\"\n",
        "    \"Step 2: Try to combine them logically to reach 110.\\n\"\n",
        "    \"Now, provide the final equation and the answer.\"\n",
        ")\n",
        "\n",
        "# Run Zero-Shot\n",
        "response_zero = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": zero_shot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Run Chain-of-Thought\n",
        "response_cot = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": cot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Display Results\n",
        "print(\"üîπ Zero-Shot Response (No Reasoning Encouraged):\\n\" + \"-\" * 50)\n",
        "print(response_zero.choices[0].message.content.strip())\n",
        "\n",
        "print(\"\\nüîπ Chain-of-Thought Response (Reasoning Encouraged):\\n\" + \"-\" * 50)\n",
        "print(response_cot.choices[0].message.content.strip())\n"
      ],
      "metadata": {
        "id": "rNdqf6qGnJY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úã Hands-On Experiment: Observations  \n",
        "\n",
        "üìå **Instructions:**  \n",
        "- Run your experiments by changing the model type (e.g., `gpt-3.5-turbo`, `gpt-4-turbo`, `gpt-o3`), temperature, and prompt style.  \n",
        "- You can **either attach a screenshot/image of your results** or **write a brief summary of your observations (max half a page)**.\n",
        "\n",
        "---\n",
        "\n",
        "- **Model Used:**  \n",
        "  _[Enter the model name you tried, e.g., gpt-3.5-turbo, gpt-4-turbo, or gpt-o3]_\n",
        "\n",
        "- **Temperature Setting:**  \n",
        "  _[Enter the temperature you used, e.g., 0.0, 0.5, 0.7]_\n",
        "\n",
        "- **Zero-Shot Result:**  \n",
        "  _[Did Zero-Shot solve the problem correctly? Yes/No. Add a short explanation or attach an image.]_\n",
        "\n",
        "- **Chain-of-Thought Result:**  \n",
        "  _[Did Chain-of-Thought solve the problem better? Yes/No. Add a short explanation or attach an image.]_\n",
        "\n",
        "- **Key Takeaways (Max Half Page or Screenshot):**  \n",
        "  _[Summarize what you observed. Did a specific model perform better? How did temperature affect the results? What worked best? Attach image or write here.]_\n",
        "\n",
        "---\n",
        "\n",
        "‚úçÔ∏è *Try at least two models and different temperatures. Compare the results and reflect on how prompting strategies influence performance!*\n"
      ],
      "metadata": {
        "id": "LQ6og568laaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÅ Self-Consistency Prompting\n",
        "\n",
        "While Chain-of-Thought (CoT) improves reasoning by encouraging step-by-step thinking, it may still produce **inconsistent or incorrect** answers, especially in complex scenarios.  \n",
        "**Self-Consistency Prompting** enhances CoT by asking the model to **generate multiple reasoning paths** and then select the most common or consistent final answer.\n",
        "\n",
        "### Why is Self-Consistency Useful?\n",
        "\n",
        "- ‚úÖ Reduces random reasoning errors.\n",
        "- ‚úÖ Boosts reliability on ambiguous or multi-path problems.\n",
        "- ‚úÖ Often improves performance on mathematical, logical, and symbolic tasks.\n",
        "\n",
        "üìñ **Reference**: [Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://arxiv.org/abs/2203.11171)\n",
        "\n",
        "---\n",
        "\n",
        "*Next, we‚Äôll see how Self-Consistency works in action using a complex reasoning example.*\n"
      ],
      "metadata": {
        "id": "RN2Af6nAkKi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# üìå Comparing Chain-of-Thought vs. Self-Consistency Prompting\n",
        "# ==========================\n",
        "\n",
        "model_name = \"gpt-4-turbo\"  # Using GPT-4 for better reasoning\n",
        "\n",
        "# Define the problem prompt\n",
        "problem_prompt = (\n",
        "    \"If a train travels at 60 miles per hour and leaves at 2 PM, and another train leaves \"\n",
        "    \"the same station at 3 PM traveling at 90 miles per hour, when will the second train catch up to the first?\"\n",
        ")\n",
        "\n",
        "# Chain-of-Thought Prompt (Standard)\n",
        "cot_prompt = (\n",
        "    \"Let's solve this step by step.\\n\"\n",
        "    + problem_prompt\n",
        ")\n",
        "\n",
        "# Self-Consistency Prompt: Ask the model to produce multiple reasoning paths\n",
        "def run_self_consistency(prompt, num_attempts=5):\n",
        "    answers = []\n",
        "    for _ in range(num_attempts):\n",
        "        response = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0.7  # Add randomness to explore different reasoning paths\n",
        "        )\n",
        "        answer = response.choices[0].message.content.strip()\n",
        "        answers.append(answer)\n",
        "    return answers\n",
        "\n",
        "# Run Chain-of-Thought (Single Attempt)\n",
        "response_cot = client.chat.completions.create(\n",
        "    model=model_name,\n",
        "    messages=[{\"role\": \"user\", \"content\": cot_prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "cot_answer = response_cot.choices[0].message.content.strip()\n",
        "\n",
        "# Run Self-Consistency (Multiple Attempts)\n",
        "sc_answers = run_self_consistency(cot_prompt, num_attempts=5)\n",
        "\n",
        "# Simple Majority Vote to Find Most Consistent Answer\n",
        "from collections import Counter\n",
        "most_common_answer = Counter(sc_answers).most_common(1)[0]\n",
        "\n",
        "# Display Results\n",
        "print(\"üîπ Chain-of-Thought Response (Single Attempt):\\n\" + \"-\" * 50)\n",
        "print(cot_answer)\n",
        "\n",
        "print(\"\\nüîπ Self-Consistency Responses (Multiple Attempts):\\n\" + \"-\" * 50)\n",
        "for idx, ans in enumerate(sc_answers, 1):\n",
        "    print(f\"Attempt {idx}: {ans}\")\n",
        "\n",
        "print(\"\\nüîπ Final Self-Consistency Selected Answer:\\n\" + \"-\" * 50)\n",
        "print(f\"Most Common Answer: {most_common_answer[0]}\\nAppeared {most_common_answer[1]} times.\")\n"
      ],
      "metadata": {
        "id": "yNWwXIahdaOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; border-radius: 12px; text-align: center;\">\n",
        "    <h1 style=\"margin-bottom: 10px;\">üìö Exploring More Advanced Prompting Strategies</h1>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4; margin-top: 20px;\">\n",
        "    <ul style=\"font-size: 16px; line-height: 1.8;\">\n",
        "        <li><strong>üß© Tree-of-Thought (ToT) Prompting:</strong> Explores multiple reasoning paths like a decision tree, helping the model evaluate and compare various solutions before choosing the best one.</li>\n",
        "        <li><strong>ü§ñ ReAct (Reasoning and Acting) Prompting:</strong> Combines reasoning steps with actions, including API calls or external tool usage. Ideal for interactive agents and dynamic decision-making tasks.</li>\n",
        "        <li><strong>üîÑ Reflexion Prompting:</strong> Encourages the model to critique its own responses and iteratively improve them, simulating self-correction and learning.</li>\n",
        "    </ul>\n",
        "</div>\n",
        "\n",
        "<div style=\"margin-top: 40px; text-align: center;\">\n",
        "    <h2 style=\"color: #001a70;\">‚úã Hands-On Task: Compare Prompting Strategies</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f5faff; padding: 20px; border-radius: 8px; border-left: 5px solid #0055d4;\">\n",
        "    <p style=\"font-size: 16px;\">\n",
        "        üìå <strong>Task Instructions:</strong><br>\n",
        "        - Experiment with <strong>Self-Consistency</strong>, <strong>Tree-of-Thought</strong>, and <strong>ReAct</strong> prompting methods.<br>\n",
        "        - Try to solve the following problem using each method and compare the results.\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4; margin-top: 20px;\">\n",
        "    <h3>üß† <strong>Challenge Problem:</strong></h3>\n",
        "    <p style=\"font-size: 16px;\">A farmer has chickens and rabbits in a cage. There are 35 heads and 94 legs. How many chickens and rabbits are there?</p>\n",
        "</div>\n",
        "\n",
        "<div style=\"margin-top: 40px;\">\n",
        "    <ul style=\"font-size: 16px; line-height: 1.8;\">\n",
        "        <li>Try different models (e.g., <code>gpt-3.5-turbo</code>, <code>gpt-4-turbo</code>, <code>gpt-o3</code>).</li>\n",
        "        <li>Experiment with different temperatures (e.g., <code>0.0</code>, <code>0.5</code>, <code>0.7</code>).</li>\n",
        "        <li>Use both direct prompts and advanced strategies like CoT, Self-Consistency, or ReAct.</li>\n",
        "    </ul>\n",
        "</div>\n",
        "\n",
        "<div style=\"margin-top: 40px; text-align: center;\">\n",
        "    <h2 style=\"color: #001a70;\">üìñ Observations</h2>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4;\">\n",
        "    <ul style=\"font-size: 16px; line-height: 1.8;\">\n",
        "        <li><strong>Model and Strategy Used:</strong><br>_[Enter the model and prompting strategy you tried]_</li>\n",
        "        <li><strong>Was the Correct Answer Found?</strong><br>_[Yes/No. Explain briefly or attach a screenshot]_</li>\n",
        "        <li><strong>Key Takeaways (Max Half Page or Screenshot):</strong><br>_[Summarize how different strategies performed. What worked best? Why?]_</li>\n",
        "    </ul>\n",
        "</div>\n",
        "\n",
        "<div style=\"margin-top: 20px; text-align: center;\">\n",
        "    ‚úçÔ∏è <em>Hint: Try breaking down the problem into equations or ask the model to explain its steps before giving the final answer. Notice which strategies lead to faster and more accurate results!</em>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "Hfw5kDf5l_o_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# ‚úã Hands-On Code: Try Different Prompting Strategies and Models\n",
        "# ==========================\n",
        "\n",
        "# üìù Instructions:\n",
        "# - Change 'model_name' to try different models (e.g., \"gpt-3.5-turbo\", \"gpt-4-turbo\", \"gpt-o3\").\n",
        "# - Adjust 'temperature' to test how creativity affects reasoning.\n",
        "# - Try Self-Consistency by sampling multiple outputs and comparing answers.\n",
        "# - Optionally, explore Tree-of-Thought and ReAct patterns by modifying prompts.\n",
        "# ‚úÖ Your Experiment Starts Here üëá\n"
      ],
      "metadata": {
        "id": "5kYqO4FgkJd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background: linear-gradient(135deg, #001a70 0%, #0055d4 100%); color: white; padding: 25px; border-radius: 12px; text-align: center;\">\n",
        "    <h1 style=\"margin-bottom: 10px;\">üìå Conclusion</h1>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4; margin-top: 20px;\">\n",
        "    <p style=\"font-size: 16px; line-height: 1.8;\">\n",
        "        In this hands-on exploration, different advanced prompting strategies were tested to solve reasoning-based challenges.\n",
        "        Through experimenting with <strong>Chain-of-Thought (CoT)</strong>, <strong>Self-Consistency</strong>, and other methods,\n",
        "        the following key insights were observed:\n",
        "    </p>\n",
        "    <ul style=\"font-size: 16px; line-height: 1.8;\">\n",
        "        <li>Advanced prompting techniques significantly improve model performance, especially on complex, multi-step problems.</li>\n",
        "        <li>Changing the <strong>model type</strong> and <strong>temperature</strong> can drastically affect reasoning quality and creativity.</li>\n",
        "        <li>Some strategies, like <strong>Self-Consistency</strong>, help reduce random errors by exploring multiple reasoning paths.</li>\n",
        "        <li>For ambiguous or challenging problems, combining strategies (e.g., CoT + Self-Consistency) often leads to the most reliable results.</li>\n",
        "    </ul>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #f5faff; padding: 20px; border-radius: 8px; border-left: 5px solid #0055d4; margin-top: 20px;\">\n",
        "    <p style=\"font-size: 16px; font-style: italic;\">\n",
        "        üìñ <em>Remember: Prompt engineering is both an art and a science. The more you experiment, the better you understand how to guide LLMs effectively!</em>\n",
        "    </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"margin-top: 40px; text-align: center;\">\n",
        "    <h3 style=\"color: #001a70;\">‚úçÔ∏è Final Reflection</h3>\n",
        "</div>\n",
        "\n",
        "<div style=\"background: #ffffff; padding: 20px; border-radius: 10px; border-left: 6px solid #0055d4;\">\n",
        "    <p style=\"font-size: 16px;\">\n",
        "        _[Write 2-3 sentences summarizing what you personally learned about prompting strategies and how model selection or temperature influenced the results.]_\n",
        "    </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "sivjpWXsmlUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2WJRwTsOmlrs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}